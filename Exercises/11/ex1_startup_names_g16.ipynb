{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-207wqx7KS4E"
   },
   "source": [
    "**g16**\n",
    "- anderdav@students.zhaw.ch\n",
    "- goodnic1@students.zhaw.ch\n",
    "- janick.rueegger@students.fhnw.ch\n",
    "- neyerjoh@students.zhaw.ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded a corpus of 2567986 characters\n"
     ]
    }
   ],
   "source": [
    "with open(\"companies.csv\") as corpus_file:\n",
    "    corpus = corpus_file.read()\n",
    "    corpus_length = len(corpus)\n",
    "    \n",
    "print(f\"Loaded a corpus of {corpus_length} characters\")\n",
    "\n",
    "#np.loadtxt(\"companies.csv\", dtype=str, delimiter=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our corpus contains 150 unique characters.\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(corpus)))\n",
    "num_chars = len(chars)\n",
    "encoding = {c: i for i, c in enumerate(chars)}\n",
    "decoding = {i: c for i, c in enumerate(chars)}\n",
    "print(\"Our corpus contains {0} unique characters.\".format(num_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '\"': 3, '#': 4, '$': 5, '%': 6, '&': 7, \"'\": 8, '(': 9, ')': 10, '*': 11, '+': 12, ',': 13, '-': 14, '.': 15, '/': 16, '0': 17, '1': 18, '2': 19, '3': 20, '4': 21, '5': 22, '6': 23, '7': 24, '8': 25, '9': 26, ':': 27, '=': 28, '>': 29, '?': 30, '@': 31, 'A': 32, 'B': 33, 'C': 34, 'D': 35, 'E': 36, 'F': 37, 'G': 38, 'H': 39, 'I': 40, 'J': 41, 'K': 42, 'L': 43, 'M': 44, 'N': 45, 'O': 46, 'P': 47, 'Q': 48, 'R': 49, 'S': 50, 'T': 51, 'U': 52, 'V': 53, 'W': 54, 'X': 55, 'Y': 56, 'Z': 57, '[': 58, '\\\\': 59, ']': 60, '^': 61, '_': 62, 'a': 63, 'b': 64, 'c': 65, 'd': 66, 'e': 67, 'f': 68, 'g': 69, 'h': 70, 'i': 71, 'j': 72, 'k': 73, 'l': 74, 'm': 75, 'n': 76, 'o': 77, 'p': 78, 'q': 79, 'r': 80, 's': 81, 't': 82, 'u': 83, 'v': 84, 'w': 85, 'x': 86, 'y': 87, 'z': 88, '{': 89, '|': 90, '}': 91, '~': 92, '\\x81': 93, '\\x82': 94, '\\x83': 95, '\\x84': 96, '\\x85': 97, '\\x86': 98, '\\x87': 99, '\\x88': 100, '\\x89': 101, '\\x8a': 102, '\\x8b': 103, '\\x8c': 104, '\\x8d': 105, '\\x8e': 106, '\\x8f': 107, '\\x90': 108, '\\x91': 109, '\\x92': 110, '\\x93': 111, '\\x94': 112, '\\x95': 113, '\\x96': 114, '\\x97': 115, '\\x98': 116, '\\x99': 117, '\\x9a': 118, '\\x9b': 119, '\\x9c': 120, '\\x9f': 121, '¡': 122, '¥': 123, '§': 124, '¨': 125, '©': 126, 'ª': 127, '«': 128, '¯': 129, 'µ': 130, '¾': 131, '¿': 132, 'Ê': 133, 'Ë': 134, 'Ì': 135, 'Ï': 136, 'Ð': 137, 'Ñ': 138, 'Ô': 139, 'Õ': 140, 'ç': 141, 'è': 142, 'ê': 143, 'ì': 144, 'î': 145, 'ï': 146, 'ò': 147, 'õ': 148, 'û': 149}\n",
      "{0: '\\n', 1: ' ', 2: '!', 3: '\"', 4: '#', 5: '$', 6: '%', 7: '&', 8: \"'\", 9: '(', 10: ')', 11: '*', 12: '+', 13: ',', 14: '-', 15: '.', 16: '/', 17: '0', 18: '1', 19: '2', 20: '3', 21: '4', 22: '5', 23: '6', 24: '7', 25: '8', 26: '9', 27: ':', 28: '=', 29: '>', 30: '?', 31: '@', 32: 'A', 33: 'B', 34: 'C', 35: 'D', 36: 'E', 37: 'F', 38: 'G', 39: 'H', 40: 'I', 41: 'J', 42: 'K', 43: 'L', 44: 'M', 45: 'N', 46: 'O', 47: 'P', 48: 'Q', 49: 'R', 50: 'S', 51: 'T', 52: 'U', 53: 'V', 54: 'W', 55: 'X', 56: 'Y', 57: 'Z', 58: '[', 59: '\\\\', 60: ']', 61: '^', 62: '_', 63: 'a', 64: 'b', 65: 'c', 66: 'd', 67: 'e', 68: 'f', 69: 'g', 70: 'h', 71: 'i', 72: 'j', 73: 'k', 74: 'l', 75: 'm', 76: 'n', 77: 'o', 78: 'p', 79: 'q', 80: 'r', 81: 's', 82: 't', 83: 'u', 84: 'v', 85: 'w', 86: 'x', 87: 'y', 88: 'z', 89: '{', 90: '|', 91: '}', 92: '~', 93: '\\x81', 94: '\\x82', 95: '\\x83', 96: '\\x84', 97: '\\x85', 98: '\\x86', 99: '\\x87', 100: '\\x88', 101: '\\x89', 102: '\\x8a', 103: '\\x8b', 104: '\\x8c', 105: '\\x8d', 106: '\\x8e', 107: '\\x8f', 108: '\\x90', 109: '\\x91', 110: '\\x92', 111: '\\x93', 112: '\\x94', 113: '\\x95', 114: '\\x96', 115: '\\x97', 116: '\\x98', 117: '\\x99', 118: '\\x9a', 119: '\\x9b', 120: '\\x9c', 121: '\\x9f', 122: '¡', 123: '¥', 124: '§', 125: '¨', 126: '©', 127: 'ª', 128: '«', 129: '¯', 130: 'µ', 131: '¾', 132: '¿', 133: 'Ê', 134: 'Ë', 135: 'Ì', 136: 'Ï', 137: 'Ð', 138: 'Ñ', 139: 'Ô', 140: 'Õ', 141: 'ç', 142: 'è', 143: 'ê', 144: 'ì', 145: 'î', 146: 'ï', 147: 'ò', 148: 'õ', 149: 'û'}\n"
     ]
    }
   ],
   "source": [
    "print(encoding)\n",
    "print(decoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Many to One  approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sliced our corpus into 855992 sequences of length 10\n"
     ]
    }
   ],
   "source": [
    "sequence_length = 10\n",
    "skip = 3\n",
    "X_data = []\n",
    "y_data = []\n",
    "for i in range (0, len(corpus) - sequence_length, skip):\n",
    "    sequence = corpus[i:i+sequence_length]\n",
    "    next_char = corpus[i+sequence_length]\n",
    "    X_data.append([encoding[char] for char in sequence])\n",
    "    y_data.append(encoding[next_char])\n",
    "\n",
    "num_sequences = len(X_data)\n",
    "print(f\"Sliced our corpus into {num_sequences} sequences of length {sequence_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39, 63, 81, 70, 78, 74, 63, 87, 1, 40]\n",
      "['H', 'a', 's', 'h', 'p', 'l', 'a', 'y', ' ', 'I']\n",
      "n\n"
     ]
    }
   ],
   "source": [
    "print(X_data[0])\n",
    "print([decoding[c] for c in X_data[0]])\n",
    "print(decoding[y_data[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# sequences: 855992\n",
      "Characters in corpus: 150\n",
      "sequence length: 10\n",
      "Sanity check y. Dimension: torch.Size([855992])\n",
      "Sanity check X. Dimension: torch.Size([855992, 10, 150])\n"
     ]
    }
   ],
   "source": [
    "X = F.one_hot(torch.tensor(X_data), num_classes=num_chars).to(torch.float)\n",
    "y = torch.tensor(y_data)\n",
    "\n",
    "print(f\"# sequences: {num_sequences}\")\n",
    "print(f\"Characters in corpus: {len(chars)}\")\n",
    "print(f\"sequence length: {sequence_length}\")\n",
    "print(f\"Sanity check y. Dimension: {y.shape}\")\n",
    "print(f\"Sanity check X. Dimension: {X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, layer_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, layer_size, batch_first=True)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, num_classes),\n",
    "            nn.Softmax(dim=-1),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        out = out[:, -1]\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "RNNModel                                 --                        --\n",
       "├─RNN: 1-1                               [128, 10, 64]             13,824\n",
       "├─Sequential: 1-2                        [128, 150]                --\n",
       "│    └─Linear: 2-1                       [128, 150]                9,750\n",
       "│    └─Softmax: 2-2                      [128, 150]                --\n",
       "==========================================================================================\n",
       "Total params: 23,574\n",
       "Trainable params: 23,574\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 18.94\n",
       "==========================================================================================\n",
       "Input size (MB): 0.77\n",
       "Forward/backward pass size (MB): 0.81\n",
       "Params size (MB): 0.09\n",
       "Estimated Total Size (MB): 1.67\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_size = 64\n",
    "num_classes = num_chars\n",
    "layer_size = 1\n",
    "batch_size= 128\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = RNNModel(num_classes, hidden_size, layer_size, num_classes).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "summary(model, input_size=(batch_size, sequence_length, num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNModel(\n",
       "  (rnn): RNN(150, 64, batch_first=True)\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=150, bias=True)\n",
       "    (1): Softmax(dim=-1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, data_loader, log_interval=200):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    total_correct = 0 \n",
    "    # Loop over each batch from the training set\n",
    "    for batch_idx, (data, target) in enumerate(tqdm(data_loader, desc=f\"Training Epoch {epoch}\")):\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        optimizer.zero_grad() \n",
    "        # Pass data through the network\n",
    "        output = model(data)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(output, target)\n",
    "        # Backpropagate. Updates the gradients buffer on each parameter\n",
    "        loss.backward()\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "        \n",
    "        _, pred = torch.max(output, dim=1)\n",
    "\n",
    "        total_correct += torch.sum(pred == target).item()\n",
    "                  \n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(data_loader.dataset),\n",
    "                100. * batch_idx / len(data_loader), loss.data.item()))\n",
    "    \n",
    "    accuracy_train = total_correct / len(data_loader.dataset)\n",
    "\n",
    "    \n",
    "    total_train_loss = total_train_loss / len(data_loader)\n",
    "\n",
    "    return {\n",
    "        \"loss\": total_train_loss,\n",
    "        \"accuracy\": accuracy_train,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode() \n",
    "def predict(model, data):\n",
    "    model.eval()\n",
    "    output = model(data)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(X, y)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "682f18c574c44d42b451e6ede0f4c3ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 1:   0%|          | 0/6688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/855992 (0%)]\tLoss: 5.010685\n",
      "Train Epoch: 1 [25600/855992 (3%)]\tLoss: 4.936292\n",
      "Train Epoch: 1 [51200/855992 (6%)]\tLoss: 4.920770\n",
      "Train Epoch: 1 [76800/855992 (9%)]\tLoss: 4.889464\n",
      "Train Epoch: 1 [102400/855992 (12%)]\tLoss: 4.943903\n",
      "Train Epoch: 1 [128000/855992 (15%)]\tLoss: 4.865936\n",
      "Train Epoch: 1 [153600/855992 (18%)]\tLoss: 4.959515\n",
      "Train Epoch: 1 [179200/855992 (21%)]\tLoss: 4.982931\n",
      "Train Epoch: 1 [204800/855992 (24%)]\tLoss: 4.951705\n",
      "Train Epoch: 1 [230400/855992 (27%)]\tLoss: 4.928297\n",
      "Train Epoch: 1 [256000/855992 (30%)]\tLoss: 4.951712\n",
      "Train Epoch: 1 [281600/855992 (33%)]\tLoss: 4.920482\n",
      "Train Epoch: 1 [307200/855992 (36%)]\tLoss: 4.920481\n",
      "Train Epoch: 1 [332800/855992 (39%)]\tLoss: 4.975141\n",
      "Train Epoch: 1 [358400/855992 (42%)]\tLoss: 4.959520\n",
      "Train Epoch: 1 [384000/855992 (45%)]\tLoss: 4.920467\n",
      "Train Epoch: 1 [409600/855992 (48%)]\tLoss: 4.951710\n",
      "Train Epoch: 1 [435200/855992 (51%)]\tLoss: 4.982957\n",
      "Train Epoch: 1 [460800/855992 (54%)]\tLoss: 4.928284\n",
      "Train Epoch: 1 [486400/855992 (57%)]\tLoss: 4.967336\n",
      "Train Epoch: 1 [512000/855992 (60%)]\tLoss: 4.959525\n",
      "Train Epoch: 1 [537600/855992 (63%)]\tLoss: 4.959525\n",
      "Train Epoch: 1 [563200/855992 (66%)]\tLoss: 4.920469\n",
      "Train Epoch: 1 [588800/855992 (69%)]\tLoss: 4.920465\n",
      "Train Epoch: 1 [614400/855992 (72%)]\tLoss: 4.943898\n",
      "Train Epoch: 1 [640000/855992 (75%)]\tLoss: 4.889219\n",
      "Train Epoch: 1 [665600/855992 (78%)]\tLoss: 4.912656\n",
      "Train Epoch: 1 [691200/855992 (81%)]\tLoss: 4.936090\n",
      "Train Epoch: 1 [716800/855992 (84%)]\tLoss: 4.982954\n",
      "Train Epoch: 1 [742400/855992 (87%)]\tLoss: 4.949894\n",
      "Train Epoch: 1 [768000/855992 (90%)]\tLoss: 4.878469\n",
      "Train Epoch: 1 [793600/855992 (93%)]\tLoss: 4.889988\n",
      "Train Epoch: 1 [819200/855992 (96%)]\tLoss: 4.853954\n",
      "Train Epoch: 1 [844800/855992 (99%)]\tLoss: 4.759303\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58534a5db53b4740bf1bbd32831bc46a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 2:   0%|          | 0/6688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [0/855992 (0%)]\tLoss: 4.833351\n",
      "Train Epoch: 2 [25600/855992 (3%)]\tLoss: 4.848730\n",
      "Train Epoch: 2 [51200/855992 (6%)]\tLoss: 4.791923\n",
      "Train Epoch: 2 [76800/855992 (9%)]\tLoss: 4.829117\n",
      "Train Epoch: 2 [102400/855992 (12%)]\tLoss: 4.808399\n",
      "Train Epoch: 2 [128000/855992 (15%)]\tLoss: 4.780657\n",
      "Train Epoch: 2 [153600/855992 (18%)]\tLoss: 4.826577\n",
      "Train Epoch: 2 [179200/855992 (21%)]\tLoss: 4.840570\n",
      "Train Epoch: 2 [204800/855992 (24%)]\tLoss: 4.753696\n",
      "Train Epoch: 2 [230400/855992 (27%)]\tLoss: 4.740283\n",
      "Train Epoch: 2 [256000/855992 (30%)]\tLoss: 4.769109\n",
      "Train Epoch: 2 [281600/855992 (33%)]\tLoss: 4.792164\n",
      "Train Epoch: 2 [307200/855992 (36%)]\tLoss: 4.820244\n",
      "Train Epoch: 2 [332800/855992 (39%)]\tLoss: 4.769356\n",
      "Train Epoch: 2 [358400/855992 (42%)]\tLoss: 4.796368\n",
      "Train Epoch: 2 [384000/855992 (45%)]\tLoss: 4.802536\n",
      "Train Epoch: 2 [409600/855992 (48%)]\tLoss: 4.763190\n",
      "Train Epoch: 2 [435200/855992 (51%)]\tLoss: 4.827420\n",
      "Train Epoch: 2 [460800/855992 (54%)]\tLoss: 4.828749\n",
      "Train Epoch: 2 [486400/855992 (57%)]\tLoss: 4.775844\n",
      "Train Epoch: 2 [512000/855992 (60%)]\tLoss: 4.764451\n",
      "Train Epoch: 2 [537600/855992 (63%)]\tLoss: 4.765857\n",
      "Train Epoch: 2 [563200/855992 (66%)]\tLoss: 4.719641\n",
      "Train Epoch: 2 [588800/855992 (69%)]\tLoss: 4.735725\n",
      "Train Epoch: 2 [614400/855992 (72%)]\tLoss: 4.786858\n",
      "Train Epoch: 2 [640000/855992 (75%)]\tLoss: 4.776846\n",
      "Train Epoch: 2 [665600/855992 (78%)]\tLoss: 4.732688\n",
      "Train Epoch: 2 [691200/855992 (81%)]\tLoss: 4.823334\n",
      "Train Epoch: 2 [716800/855992 (84%)]\tLoss: 4.731277\n",
      "Train Epoch: 2 [742400/855992 (87%)]\tLoss: 4.733921\n",
      "Train Epoch: 2 [768000/855992 (90%)]\tLoss: 4.784825\n",
      "Train Epoch: 2 [793600/855992 (93%)]\tLoss: 4.799022\n",
      "Train Epoch: 2 [819200/855992 (96%)]\tLoss: 4.839867\n",
      "Train Epoch: 2 [844800/855992 (99%)]\tLoss: 4.729705\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d2f9422343846feb607d0a6e1c004fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 3:   0%|          | 0/6688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [0/855992 (0%)]\tLoss: 4.774034\n",
      "Train Epoch: 3 [25600/855992 (3%)]\tLoss: 4.700772\n",
      "Train Epoch: 3 [51200/855992 (6%)]\tLoss: 4.844039\n",
      "Train Epoch: 3 [76800/855992 (9%)]\tLoss: 4.812463\n",
      "Train Epoch: 3 [102400/855992 (12%)]\tLoss: 4.767422\n",
      "Train Epoch: 3 [128000/855992 (15%)]\tLoss: 4.791334\n",
      "Train Epoch: 3 [153600/855992 (18%)]\tLoss: 4.732358\n",
      "Train Epoch: 3 [179200/855992 (21%)]\tLoss: 4.693199\n",
      "Train Epoch: 3 [204800/855992 (24%)]\tLoss: 4.677909\n",
      "Train Epoch: 3 [230400/855992 (27%)]\tLoss: 4.783278\n",
      "Train Epoch: 3 [256000/855992 (30%)]\tLoss: 4.801521\n",
      "Train Epoch: 3 [281600/855992 (33%)]\tLoss: 4.788512\n",
      "Train Epoch: 3 [307200/855992 (36%)]\tLoss: 4.797509\n",
      "Train Epoch: 3 [332800/855992 (39%)]\tLoss: 4.797469\n",
      "Train Epoch: 3 [358400/855992 (42%)]\tLoss: 4.706284\n",
      "Train Epoch: 3 [384000/855992 (45%)]\tLoss: 4.760263\n",
      "Train Epoch: 3 [409600/855992 (48%)]\tLoss: 4.758116\n",
      "Train Epoch: 3 [435200/855992 (51%)]\tLoss: 4.718832\n",
      "Train Epoch: 3 [460800/855992 (54%)]\tLoss: 4.728637\n",
      "Train Epoch: 3 [486400/855992 (57%)]\tLoss: 4.785521\n",
      "Train Epoch: 3 [512000/855992 (60%)]\tLoss: 4.768130\n",
      "Train Epoch: 3 [537600/855992 (63%)]\tLoss: 4.765690\n",
      "Train Epoch: 3 [563200/855992 (66%)]\tLoss: 4.749022\n",
      "Train Epoch: 3 [588800/855992 (69%)]\tLoss: 4.753874\n",
      "Train Epoch: 3 [614400/855992 (72%)]\tLoss: 4.731264\n",
      "Train Epoch: 3 [640000/855992 (75%)]\tLoss: 4.705436\n",
      "Train Epoch: 3 [665600/855992 (78%)]\tLoss: 4.733509\n",
      "Train Epoch: 3 [691200/855992 (81%)]\tLoss: 4.760328\n",
      "Train Epoch: 3 [716800/855992 (84%)]\tLoss: 4.773803\n",
      "Train Epoch: 3 [742400/855992 (87%)]\tLoss: 4.766563\n",
      "Train Epoch: 3 [768000/855992 (90%)]\tLoss: 4.759611\n",
      "Train Epoch: 3 [793600/855992 (93%)]\tLoss: 4.716615\n",
      "Train Epoch: 3 [819200/855992 (96%)]\tLoss: 4.746736\n",
      "Train Epoch: 3 [844800/855992 (99%)]\tLoss: 4.762456\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d5b161a91414929add71da5ea227902",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 4:   0%|          | 0/6688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [0/855992 (0%)]\tLoss: 4.786294\n",
      "Train Epoch: 4 [25600/855992 (3%)]\tLoss: 4.747069\n",
      "Train Epoch: 4 [51200/855992 (6%)]\tLoss: 4.809166\n",
      "Train Epoch: 4 [76800/855992 (9%)]\tLoss: 4.744350\n",
      "Train Epoch: 4 [102400/855992 (12%)]\tLoss: 4.730820\n",
      "Train Epoch: 4 [128000/855992 (15%)]\tLoss: 4.811648\n",
      "Train Epoch: 4 [153600/855992 (18%)]\tLoss: 4.669909\n",
      "Train Epoch: 4 [179200/855992 (21%)]\tLoss: 4.727087\n",
      "Train Epoch: 4 [204800/855992 (24%)]\tLoss: 4.729530\n",
      "Train Epoch: 4 [230400/855992 (27%)]\tLoss: 4.734234\n",
      "Train Epoch: 4 [256000/855992 (30%)]\tLoss: 4.736125\n",
      "Train Epoch: 4 [281600/855992 (33%)]\tLoss: 4.799965\n",
      "Train Epoch: 4 [307200/855992 (36%)]\tLoss: 4.736814\n",
      "Train Epoch: 4 [332800/855992 (39%)]\tLoss: 4.710070\n",
      "Train Epoch: 4 [358400/855992 (42%)]\tLoss: 4.733772\n",
      "Train Epoch: 4 [384000/855992 (45%)]\tLoss: 4.721423\n",
      "Train Epoch: 4 [409600/855992 (48%)]\tLoss: 4.675649\n",
      "Train Epoch: 4 [435200/855992 (51%)]\tLoss: 4.726267\n",
      "Train Epoch: 4 [460800/855992 (54%)]\tLoss: 4.712678\n",
      "Train Epoch: 4 [486400/855992 (57%)]\tLoss: 4.731094\n",
      "Train Epoch: 4 [512000/855992 (60%)]\tLoss: 4.748594\n",
      "Train Epoch: 4 [537600/855992 (63%)]\tLoss: 4.775200\n",
      "Train Epoch: 4 [563200/855992 (66%)]\tLoss: 4.704687\n",
      "Train Epoch: 4 [588800/855992 (69%)]\tLoss: 4.738225\n",
      "Train Epoch: 4 [614400/855992 (72%)]\tLoss: 4.760039\n",
      "Train Epoch: 4 [640000/855992 (75%)]\tLoss: 4.726427\n",
      "Train Epoch: 4 [665600/855992 (78%)]\tLoss: 4.691209\n",
      "Train Epoch: 4 [691200/855992 (81%)]\tLoss: 4.813902\n",
      "Train Epoch: 4 [716800/855992 (84%)]\tLoss: 4.696117\n",
      "Train Epoch: 4 [742400/855992 (87%)]\tLoss: 4.714275\n",
      "Train Epoch: 4 [768000/855992 (90%)]\tLoss: 4.674923\n",
      "Train Epoch: 4 [793600/855992 (93%)]\tLoss: 4.694229\n",
      "Train Epoch: 4 [819200/855992 (96%)]\tLoss: 4.772410\n",
      "Train Epoch: 4 [844800/855992 (99%)]\tLoss: 4.708535\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd385fa0892a45df9918d0bd832998c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 5:   0%|          | 0/6688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 [0/855992 (0%)]\tLoss: 4.656658\n",
      "Train Epoch: 5 [25600/855992 (3%)]\tLoss: 4.623310\n",
      "Train Epoch: 5 [51200/855992 (6%)]\tLoss: 4.761882\n",
      "Train Epoch: 5 [76800/855992 (9%)]\tLoss: 4.694811\n",
      "Train Epoch: 5 [102400/855992 (12%)]\tLoss: 4.713318\n",
      "Train Epoch: 5 [128000/855992 (15%)]\tLoss: 4.732544\n",
      "Train Epoch: 5 [153600/855992 (18%)]\tLoss: 4.744422\n",
      "Train Epoch: 5 [179200/855992 (21%)]\tLoss: 4.678281\n",
      "Train Epoch: 5 [204800/855992 (24%)]\tLoss: 4.735976\n",
      "Train Epoch: 5 [230400/855992 (27%)]\tLoss: 4.757316\n",
      "Train Epoch: 5 [256000/855992 (30%)]\tLoss: 4.676985\n",
      "Train Epoch: 5 [281600/855992 (33%)]\tLoss: 4.721568\n",
      "Train Epoch: 5 [307200/855992 (36%)]\tLoss: 4.690185\n",
      "Train Epoch: 5 [332800/855992 (39%)]\tLoss: 4.749906\n",
      "Train Epoch: 5 [358400/855992 (42%)]\tLoss: 4.766503\n",
      "Train Epoch: 5 [384000/855992 (45%)]\tLoss: 4.732476\n",
      "Train Epoch: 5 [409600/855992 (48%)]\tLoss: 4.685112\n",
      "Train Epoch: 5 [435200/855992 (51%)]\tLoss: 4.726568\n",
      "Train Epoch: 5 [460800/855992 (54%)]\tLoss: 4.683387\n",
      "Train Epoch: 5 [486400/855992 (57%)]\tLoss: 4.704216\n",
      "Train Epoch: 5 [512000/855992 (60%)]\tLoss: 4.695778\n",
      "Train Epoch: 5 [537600/855992 (63%)]\tLoss: 4.771357\n",
      "Train Epoch: 5 [563200/855992 (66%)]\tLoss: 4.660518\n",
      "Train Epoch: 5 [588800/855992 (69%)]\tLoss: 4.641742\n",
      "Train Epoch: 5 [614400/855992 (72%)]\tLoss: 4.670096\n",
      "Train Epoch: 5 [640000/855992 (75%)]\tLoss: 4.734321\n",
      "Train Epoch: 5 [665600/855992 (78%)]\tLoss: 4.744538\n",
      "Train Epoch: 5 [691200/855992 (81%)]\tLoss: 4.766121\n",
      "Train Epoch: 5 [716800/855992 (84%)]\tLoss: 4.677106\n",
      "Train Epoch: 5 [742400/855992 (87%)]\tLoss: 4.753708\n",
      "Train Epoch: 5 [768000/855992 (90%)]\tLoss: 4.782032\n",
      "Train Epoch: 5 [793600/855992 (93%)]\tLoss: 4.804272\n",
      "Train Epoch: 5 [819200/855992 (96%)]\tLoss: 4.763226\n",
      "Train Epoch: 5 [844800/855992 (99%)]\tLoss: 4.766962\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9fffea400714b7781d99c0695b549f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 6:   0%|          | 0/6688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [0/855992 (0%)]\tLoss: 4.744456\n",
      "Train Epoch: 6 [25600/855992 (3%)]\tLoss: 4.735473\n",
      "Train Epoch: 6 [51200/855992 (6%)]\tLoss: 4.744827\n",
      "Train Epoch: 6 [76800/855992 (9%)]\tLoss: 4.688704\n",
      "Train Epoch: 6 [102400/855992 (12%)]\tLoss: 4.727985\n",
      "Train Epoch: 6 [128000/855992 (15%)]\tLoss: 4.764800\n",
      "Train Epoch: 6 [153600/855992 (18%)]\tLoss: 4.741763\n",
      "Train Epoch: 6 [179200/855992 (21%)]\tLoss: 4.663023\n",
      "Train Epoch: 6 [204800/855992 (24%)]\tLoss: 4.784701\n",
      "Train Epoch: 6 [230400/855992 (27%)]\tLoss: 4.731513\n",
      "Train Epoch: 6 [256000/855992 (30%)]\tLoss: 4.763804\n",
      "Train Epoch: 6 [281600/855992 (33%)]\tLoss: 4.739939\n",
      "Train Epoch: 6 [307200/855992 (36%)]\tLoss: 4.777192\n",
      "Train Epoch: 6 [332800/855992 (39%)]\tLoss: 4.674311\n",
      "Train Epoch: 6 [358400/855992 (42%)]\tLoss: 4.693240\n",
      "Train Epoch: 6 [384000/855992 (45%)]\tLoss: 4.713624\n",
      "Train Epoch: 6 [409600/855992 (48%)]\tLoss: 4.687284\n",
      "Train Epoch: 6 [435200/855992 (51%)]\tLoss: 4.717800\n",
      "Train Epoch: 6 [460800/855992 (54%)]\tLoss: 4.683241\n",
      "Train Epoch: 6 [486400/855992 (57%)]\tLoss: 4.707565\n",
      "Train Epoch: 6 [512000/855992 (60%)]\tLoss: 4.704767\n",
      "Train Epoch: 6 [537600/855992 (63%)]\tLoss: 4.727776\n",
      "Train Epoch: 6 [563200/855992 (66%)]\tLoss: 4.731852\n",
      "Train Epoch: 6 [588800/855992 (69%)]\tLoss: 4.701586\n",
      "Train Epoch: 6 [614400/855992 (72%)]\tLoss: 4.732165\n",
      "Train Epoch: 6 [640000/855992 (75%)]\tLoss: 4.735175\n",
      "Train Epoch: 6 [665600/855992 (78%)]\tLoss: 4.694583\n",
      "Train Epoch: 6 [691200/855992 (81%)]\tLoss: 4.751342\n",
      "Train Epoch: 6 [716800/855992 (84%)]\tLoss: 4.717845\n",
      "Train Epoch: 6 [742400/855992 (87%)]\tLoss: 4.669559\n",
      "Train Epoch: 6 [768000/855992 (90%)]\tLoss: 4.727135\n",
      "Train Epoch: 6 [793600/855992 (93%)]\tLoss: 4.800488\n",
      "Train Epoch: 6 [819200/855992 (96%)]\tLoss: 4.783208\n",
      "Train Epoch: 6 [844800/855992 (99%)]\tLoss: 4.648256\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d406092aea43401390f2b4b13ee302e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 7:   0%|          | 0/6688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [0/855992 (0%)]\tLoss: 4.767110\n",
      "Train Epoch: 7 [25600/855992 (3%)]\tLoss: 4.675468\n",
      "Train Epoch: 7 [51200/855992 (6%)]\tLoss: 4.725726\n",
      "Train Epoch: 7 [76800/855992 (9%)]\tLoss: 4.728878\n",
      "Train Epoch: 7 [102400/855992 (12%)]\tLoss: 4.707928\n",
      "Train Epoch: 7 [128000/855992 (15%)]\tLoss: 4.698589\n",
      "Train Epoch: 7 [153600/855992 (18%)]\tLoss: 4.635870\n",
      "Train Epoch: 7 [179200/855992 (21%)]\tLoss: 4.730591\n",
      "Train Epoch: 7 [204800/855992 (24%)]\tLoss: 4.780990\n",
      "Train Epoch: 7 [230400/855992 (27%)]\tLoss: 4.754541\n",
      "Train Epoch: 7 [256000/855992 (30%)]\tLoss: 4.696138\n",
      "Train Epoch: 7 [281600/855992 (33%)]\tLoss: 4.726100\n",
      "Train Epoch: 7 [307200/855992 (36%)]\tLoss: 4.695783\n",
      "Train Epoch: 7 [332800/855992 (39%)]\tLoss: 4.732627\n",
      "Train Epoch: 7 [358400/855992 (42%)]\tLoss: 4.711653\n",
      "Train Epoch: 7 [384000/855992 (45%)]\tLoss: 4.825142\n",
      "Train Epoch: 7 [409600/855992 (48%)]\tLoss: 4.686516\n",
      "Train Epoch: 7 [435200/855992 (51%)]\tLoss: 4.684203\n",
      "Train Epoch: 7 [460800/855992 (54%)]\tLoss: 4.781425\n",
      "Train Epoch: 7 [486400/855992 (57%)]\tLoss: 4.733089\n",
      "Train Epoch: 7 [512000/855992 (60%)]\tLoss: 4.698126\n",
      "Train Epoch: 7 [537600/855992 (63%)]\tLoss: 4.758352\n",
      "Train Epoch: 7 [563200/855992 (66%)]\tLoss: 4.728670\n",
      "Train Epoch: 7 [588800/855992 (69%)]\tLoss: 4.793376\n",
      "Train Epoch: 7 [614400/855992 (72%)]\tLoss: 4.692837\n",
      "Train Epoch: 7 [640000/855992 (75%)]\tLoss: 4.745566\n",
      "Train Epoch: 7 [665600/855992 (78%)]\tLoss: 4.739870\n",
      "Train Epoch: 7 [691200/855992 (81%)]\tLoss: 4.699402\n",
      "Train Epoch: 7 [716800/855992 (84%)]\tLoss: 4.796613\n",
      "Train Epoch: 7 [742400/855992 (87%)]\tLoss: 4.729156\n",
      "Train Epoch: 7 [768000/855992 (90%)]\tLoss: 4.700755\n",
      "Train Epoch: 7 [793600/855992 (93%)]\tLoss: 4.718233\n",
      "Train Epoch: 7 [819200/855992 (96%)]\tLoss: 4.706325\n",
      "Train Epoch: 7 [844800/855992 (99%)]\tLoss: 4.772771\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bddfee37fbb247d0986ad62d6cd2e116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 8:   0%|          | 0/6688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8 [0/855992 (0%)]\tLoss: 4.715795\n",
      "Train Epoch: 8 [25600/855992 (3%)]\tLoss: 4.606381\n",
      "Train Epoch: 8 [51200/855992 (6%)]\tLoss: 4.717302\n",
      "Train Epoch: 8 [76800/855992 (9%)]\tLoss: 4.672729\n",
      "Train Epoch: 8 [102400/855992 (12%)]\tLoss: 4.635515\n",
      "Train Epoch: 8 [128000/855992 (15%)]\tLoss: 4.680321\n",
      "Train Epoch: 8 [153600/855992 (18%)]\tLoss: 4.650402\n",
      "Train Epoch: 8 [179200/855992 (21%)]\tLoss: 4.690157\n",
      "Train Epoch: 8 [204800/855992 (24%)]\tLoss: 4.723172\n",
      "Train Epoch: 8 [230400/855992 (27%)]\tLoss: 4.649329\n",
      "Train Epoch: 8 [256000/855992 (30%)]\tLoss: 4.689195\n",
      "Train Epoch: 8 [281600/855992 (33%)]\tLoss: 4.701008\n",
      "Train Epoch: 8 [307200/855992 (36%)]\tLoss: 4.667573\n",
      "Train Epoch: 8 [332800/855992 (39%)]\tLoss: 4.742779\n",
      "Train Epoch: 8 [358400/855992 (42%)]\tLoss: 4.720887\n",
      "Train Epoch: 8 [384000/855992 (45%)]\tLoss: 4.727548\n",
      "Train Epoch: 8 [409600/855992 (48%)]\tLoss: 4.718256\n",
      "Train Epoch: 8 [435200/855992 (51%)]\tLoss: 4.741251\n",
      "Train Epoch: 8 [460800/855992 (54%)]\tLoss: 4.662913\n",
      "Train Epoch: 8 [486400/855992 (57%)]\tLoss: 4.766223\n",
      "Train Epoch: 8 [512000/855992 (60%)]\tLoss: 4.682280\n",
      "Train Epoch: 8 [537600/855992 (63%)]\tLoss: 4.718682\n",
      "Train Epoch: 8 [563200/855992 (66%)]\tLoss: 4.727286\n",
      "Train Epoch: 8 [588800/855992 (69%)]\tLoss: 4.701863\n",
      "Train Epoch: 8 [614400/855992 (72%)]\tLoss: 4.698652\n",
      "Train Epoch: 8 [640000/855992 (75%)]\tLoss: 4.768303\n",
      "Train Epoch: 8 [665600/855992 (78%)]\tLoss: 4.705876\n",
      "Train Epoch: 8 [691200/855992 (81%)]\tLoss: 4.712796\n",
      "Train Epoch: 8 [716800/855992 (84%)]\tLoss: 4.688899\n",
      "Train Epoch: 8 [742400/855992 (87%)]\tLoss: 4.645201\n",
      "Train Epoch: 8 [768000/855992 (90%)]\tLoss: 4.725444\n",
      "Train Epoch: 8 [793600/855992 (93%)]\tLoss: 4.664597\n",
      "Train Epoch: 8 [819200/855992 (96%)]\tLoss: 4.755505\n",
      "Train Epoch: 8 [844800/855992 (99%)]\tLoss: 4.719747\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71e27c7be3754a4d91b0bd55d60dd709",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 9:   0%|          | 0/6688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [0/855992 (0%)]\tLoss: 4.684129\n",
      "Train Epoch: 9 [25600/855992 (3%)]\tLoss: 4.694532\n",
      "Train Epoch: 9 [51200/855992 (6%)]\tLoss: 4.782624\n",
      "Train Epoch: 9 [76800/855992 (9%)]\tLoss: 4.712178\n",
      "Train Epoch: 9 [102400/855992 (12%)]\tLoss: 4.713583\n",
      "Train Epoch: 9 [128000/855992 (15%)]\tLoss: 4.635890\n",
      "Train Epoch: 9 [153600/855992 (18%)]\tLoss: 4.646721\n",
      "Train Epoch: 9 [179200/855992 (21%)]\tLoss: 4.699159\n",
      "Train Epoch: 9 [204800/855992 (24%)]\tLoss: 4.743248\n",
      "Train Epoch: 9 [230400/855992 (27%)]\tLoss: 4.711519\n",
      "Train Epoch: 9 [256000/855992 (30%)]\tLoss: 4.720269\n",
      "Train Epoch: 9 [281600/855992 (33%)]\tLoss: 4.707355\n",
      "Train Epoch: 9 [307200/855992 (36%)]\tLoss: 4.687472\n",
      "Train Epoch: 9 [332800/855992 (39%)]\tLoss: 4.702065\n",
      "Train Epoch: 9 [358400/855992 (42%)]\tLoss: 4.716147\n",
      "Train Epoch: 9 [384000/855992 (45%)]\tLoss: 4.708540\n",
      "Train Epoch: 9 [409600/855992 (48%)]\tLoss: 4.668269\n",
      "Train Epoch: 9 [435200/855992 (51%)]\tLoss: 4.723232\n",
      "Train Epoch: 9 [460800/855992 (54%)]\tLoss: 4.668338\n",
      "Train Epoch: 9 [486400/855992 (57%)]\tLoss: 4.698187\n",
      "Train Epoch: 9 [512000/855992 (60%)]\tLoss: 4.663329\n",
      "Train Epoch: 9 [537600/855992 (63%)]\tLoss: 4.673749\n",
      "Train Epoch: 9 [563200/855992 (66%)]\tLoss: 4.747656\n",
      "Train Epoch: 9 [588800/855992 (69%)]\tLoss: 4.672396\n",
      "Train Epoch: 9 [614400/855992 (72%)]\tLoss: 4.723790\n",
      "Train Epoch: 9 [640000/855992 (75%)]\tLoss: 4.767193\n",
      "Train Epoch: 9 [665600/855992 (78%)]\tLoss: 4.689525\n",
      "Train Epoch: 9 [691200/855992 (81%)]\tLoss: 4.718583\n",
      "Train Epoch: 9 [716800/855992 (84%)]\tLoss: 4.736792\n",
      "Train Epoch: 9 [742400/855992 (87%)]\tLoss: 4.694104\n",
      "Train Epoch: 9 [768000/855992 (90%)]\tLoss: 4.757160\n",
      "Train Epoch: 9 [793600/855992 (93%)]\tLoss: 4.673555\n",
      "Train Epoch: 9 [819200/855992 (96%)]\tLoss: 4.716877\n",
      "Train Epoch: 9 [844800/855992 (99%)]\tLoss: 4.719455\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1ad696868bb4972818fd2591368b521",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 10:   0%|          | 0/6688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 10 [0/855992 (0%)]\tLoss: 4.724474\n",
      "Train Epoch: 10 [25600/855992 (3%)]\tLoss: 4.737867\n",
      "Train Epoch: 10 [51200/855992 (6%)]\tLoss: 4.689765\n",
      "Train Epoch: 10 [76800/855992 (9%)]\tLoss: 4.777081\n",
      "Train Epoch: 10 [102400/855992 (12%)]\tLoss: 4.675215\n",
      "Train Epoch: 10 [128000/855992 (15%)]\tLoss: 4.732692\n",
      "Train Epoch: 10 [153600/855992 (18%)]\tLoss: 4.701852\n",
      "Train Epoch: 10 [179200/855992 (21%)]\tLoss: 4.724959\n",
      "Train Epoch: 10 [204800/855992 (24%)]\tLoss: 4.710461\n",
      "Train Epoch: 10 [230400/855992 (27%)]\tLoss: 4.718573\n",
      "Train Epoch: 10 [256000/855992 (30%)]\tLoss: 4.738539\n",
      "Train Epoch: 10 [281600/855992 (33%)]\tLoss: 4.651292\n",
      "Train Epoch: 10 [307200/855992 (36%)]\tLoss: 4.726159\n",
      "Train Epoch: 10 [332800/855992 (39%)]\tLoss: 4.673893\n",
      "Train Epoch: 10 [358400/855992 (42%)]\tLoss: 4.743781\n",
      "Train Epoch: 10 [384000/855992 (45%)]\tLoss: 4.716176\n",
      "Train Epoch: 10 [409600/855992 (48%)]\tLoss: 4.695560\n",
      "Train Epoch: 10 [435200/855992 (51%)]\tLoss: 4.708025\n",
      "Train Epoch: 10 [460800/855992 (54%)]\tLoss: 4.724327\n",
      "Train Epoch: 10 [486400/855992 (57%)]\tLoss: 4.697733\n",
      "Train Epoch: 10 [512000/855992 (60%)]\tLoss: 4.705248\n",
      "Train Epoch: 10 [537600/855992 (63%)]\tLoss: 4.689861\n",
      "Train Epoch: 10 [563200/855992 (66%)]\tLoss: 4.673869\n",
      "Train Epoch: 10 [588800/855992 (69%)]\tLoss: 4.674143\n",
      "Train Epoch: 10 [614400/855992 (72%)]\tLoss: 4.643977\n",
      "Train Epoch: 10 [640000/855992 (75%)]\tLoss: 4.682565\n",
      "Train Epoch: 10 [665600/855992 (78%)]\tLoss: 4.708655\n",
      "Train Epoch: 10 [691200/855992 (81%)]\tLoss: 4.746548\n",
      "Train Epoch: 10 [716800/855992 (84%)]\tLoss: 4.691908\n",
      "Train Epoch: 10 [742400/855992 (87%)]\tLoss: 4.805966\n",
      "Train Epoch: 10 [768000/855992 (90%)]\tLoss: 4.661006\n",
      "Train Epoch: 10 [793600/855992 (93%)]\tLoss: 4.759992\n",
      "Train Epoch: 10 [819200/855992 (96%)]\tLoss: 4.710656\n",
      "Train Epoch: 10 [844800/855992 (99%)]\tLoss: 4.740471\n",
      "CPU times: user 14min 31s, sys: 1.67 s, total: 14min 32s\n",
      "Wall time: 2min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_result = train(epoch, model, train_loader)\n",
    "    train_losses.append(train_result[\"loss\"])\n",
    "    train_accuracies.append(train_result[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed = \"loud Compu\"\n",
      "rent Sore Sore Sone\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def make_seed(seed_phrase=\"\"):\n",
    "        if seed_phrase:\n",
    "            phrase_length = len(seed_phrase)\n",
    "            pattern = \"\"\n",
    "            for i in range (0, sequence_length):\n",
    "                pattern += seed_phrase[i % phrase_length]\n",
    "        else:\n",
    "            seed = random.randint(0, corpus_length - sequence_length)\n",
    "            pattern = corpus[seed:seed + sequence_length]\n",
    "        return pattern\n",
    "\n",
    "seed_pattern = make_seed()\n",
    "print(f\"seed = \\\"{seed_pattern}\\\"\")\n",
    "\n",
    "encoded_text = torch.tensor([encoding[char] for char in seed_pattern])\n",
    "encoded_text = F.one_hot(encoded_text, num_classes=num_chars).to(torch.float)\n",
    "encoded_text = encoded_text.unsqueeze(0)\n",
    "encoded_text = encoded_text.to(device)\n",
    "\n",
    "generated_text = \"\"\n",
    "for i in range(20):\n",
    "    output = predict(model, encoded_text).squeeze()\n",
    "\n",
    "    prediction = torch.multinomial(output, num_samples=1)\n",
    "    #prediction = output.argmax()\n",
    "\n",
    "    generated_text += decoding[int(prediction)]\n",
    "  \n",
    "    next_char_encoded = F.one_hot(prediction, num_classes=num_chars)\n",
    "    next_char_encoded = next_char_encoded.view(1, 1, num_chars)\n",
    "    encoded_text = torch.cat((encoded_text[:, 1:], next_char_encoded), dim=1)\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Many to many approach\n",
    "!TO DO: the `forward()` function of the `mmRNNmodel()` below needs to be updated to fit the many2many approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sliced our corpus into 1115374 sentences of length 20\n"
     ]
    }
   ],
   "source": [
    "# chop up our data into X and y, slice into roughly \n",
    "# (num_chars / skip) overlapping 'sequences' of length \n",
    "# sequence_length, and encode the chars\n",
    "sequence_length = 20\n",
    "skip = 1\n",
    "X_data = []\n",
    "y_data = []\n",
    "for i in range (0, len(corpus) - sequence_length, skip):\n",
    "    sequence = corpus[i:i + sequence_length]\n",
    "    next_char = corpus[i+1:i+1 + sequence_length]\n",
    "    X_data.append([encoding[char] for char in sequence])\n",
    "    y_data.append([encoding[char] for char in next_char])\n",
    "\n",
    "num_sequences = len(X_data)\n",
    "print(\"Sliced our corpus into {0} sequences of length {1}\"\n",
    "      .format(num_sequences, sequence_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 14, 43, 44, 53, 56]\n"
     ]
    }
   ],
   "source": [
    "print(X_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['F', 'i', 'r', 's', 't', ' ', 'C', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'B', 'e', 'f', 'o', 'r']\n",
      "['i', 'r', 's', 't', ' ', 'C', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'B', 'e', 'f', 'o', 'r', 'e']\n"
     ]
    }
   ],
   "source": [
    "print([decoding[idx] for idx in X_data[0]])\n",
    "print([decoding[idx] for idx in y_data[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the data.\n",
    "X = F.one_hot(torch.tensor(X_data), num_classes=num_chars).to(torch.float)\n",
    "y = torch.tensor(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define the model\n",
    "class mmRNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, layer_size, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Defining the number of h layers and the nodes in each layer\n",
    "        self.layer_size = layer_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # RNN layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, layer_size, batch_first=True)\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    # TO DO: Needs to be adapted to the many2many configuration\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        out = out[:, -1]\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "mmRNNModel                               --                        --\n",
       "├─RNN: 1-1                               [128, 20, 256]            82,688\n",
       "├─Linear: 1-2                            [128, 65]                 16,705\n",
       "==========================================================================================\n",
       "Total params: 99,393\n",
       "Trainable params: 99,393\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 213.82\n",
       "==========================================================================================\n",
       "Input size (MB): 0.67\n",
       "Forward/backward pass size (MB): 5.31\n",
       "Params size (MB): 0.40\n",
       "Estimated Total Size (MB): 6.37\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "seq_length = 20\n",
    "num_classes = 65\n",
    "layer_size = 1\n",
    "batch_size= 128\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "modelmm = mmRNNModel(num_classes, hidden_size, layer_size, num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "summary(modelmm, input_size=(batch_size, seq_length, num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training time\n",
    "def train(epoch, model, data_loader, log_interval=200):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    total_correct = 0 \n",
    "    # Loop over each batch from the training set\n",
    "    for batch_idx, (data, target) in enumerate(tqdm(data_loader, desc=f\"Training Epoch {epoch}\")):\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        optimizer.zero_grad() \n",
    "        # Pass data through the network\n",
    "        output = model(data)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(output, target)\n",
    "        # Backpropagate. Updates the gradients buffer on each parameter\n",
    "        loss.backward()\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "        \n",
    "        _, pred = torch.max(output, dim=1)\n",
    "\n",
    "        total_correct += torch.sum(pred == target).item()\n",
    "                  \n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(data_loader.dataset),\n",
    "                100. * batch_idx / len(data_loader), loss.data.item()))\n",
    "    \n",
    "    accuracy_train = total_correct / len(data_loader.dataset)\n",
    "\n",
    "    \n",
    "    total_train_loss = total_train_loss / len(data_loader)\n",
    "\n",
    "    return {\n",
    "        \"loss\": total_train_loss,\n",
    "        \"accuracy\": accuracy_train,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode() \n",
    "def predict(model, data):\n",
    "    # Put the model in eval mode, which disables training specific behaviour.\n",
    "    model.eval()\n",
    "    output = model(data)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloader from X,y tensors\n",
    "datasett = TensorDataset(X, y)\n",
    "train_loader = DataLoader(datasett, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%%time` not found.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# train the model\n",
    "\n",
    "# Keep track of stats to plot them\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_result = train(epoch, modelmm, train_loader)\n",
    "    train_losses.append(train_result[\"loss\"])\n",
    "    train_accuracies.append(train_result[\"accuracy\"])\n",
    "    \n",
    "\n",
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_seed(seed_phrase=\"\"):\n",
    "        if seed_phrase:  # make sure the seed has the right length\n",
    "            phrase_length = len(seed_phrase)\n",
    "            pattern = \"\"\n",
    "            for i in range (0, sequence_length):\n",
    "                pattern += seed_phrase[i % phrase_length]\n",
    "        else:            # sample randomly the seed from corpus\n",
    "            seed = random.randint(0, corpus_length - sequence_length)\n",
    "            pattern = corpus[seed:seed + sequence_length]\n",
    "        return pattern\n",
    "\n",
    "seed_pattern = make_seed(\"In the early morning, the flower is shining\")\n",
    "\n",
    "encoded_text = torch.tensor([encoding[char] for char in seed_pattern])\n",
    "encoded_text = F.one_hot(encoded_text, num_classes=num_chars).to(torch.float)\n",
    "# Add a single batch dimension at the beginning\n",
    "encoded_text = encoded_text.unsqueeze(0)\n",
    "encoded_text = encoded_text.to(device)\n",
    "\n",
    "generated_text = \"\"\n",
    "for i in range(500):\n",
    "    output = predict(modelmm, encoded_text)[0]\n",
    "    # Convert the output to probabilities\n",
    "    probs = torch.softmax(output, dim=-1)\n",
    "    # Randomly choose from a multinomial distribution with the output probabilities\n",
    "    # make the generation more diverse.\n",
    "    prediction = torch.multinomial(probs, num_samples=1)\n",
    "    generated_text += decoding[int(prediction)]\n",
    "    \n",
    "    # One hot encode the new (predicted) character\n",
    "    next_char_encoded = F.one_hot(prediction, num_classes=num_chars)\n",
    "    # Make sure it has a singular batch and seq_len dimension in order to concatenate them.\n",
    "    next_char_encoded = next_char_encoded.view(1, 1, num_chars)\n",
    "    # Remove first char and glue the predicted one to the end\n",
    "    encoded_text = torch.cat((encoded_text[:, 1:], next_char_encoded), dim=1)\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
